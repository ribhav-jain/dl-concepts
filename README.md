# ğŸ§  Deep Learning Concepts

This repository contains well-organized, annotated code and notes covering the foundational to advanced concepts in **Deep Learning**. Itâ€™s aimed at learners, practitioners, and anyone preparing for interviews or projects in the deep learning space.

---

## ğŸ“š Contents

### ğŸ”¸ Core Topics

- ğŸ”¢ **Neural Networks**: Perceptron, Multi-layer Perceptron, Backpropagation
- ğŸ“ **Activation Functions**: Sigmoid, Tanh, ReLU, LeakyReLU, Softmax
- ğŸ§± **Optimization**: Gradient Descent, Adam, RMSProp, Learning Rate Schedules
- ğŸ§  **CNNs**: Convolutional Layers, Pooling, Filters, CNN Architectures
- ğŸ” **RNNs & LSTMs**: Sequence models, Vanishing gradients, LSTM/GRU internals
- ğŸ“‰ **Loss Functions**: MSE, Cross-Entropy, Custom Losses
- ğŸ“Š **Evaluation Metrics**: Accuracy, Precision, Recall, F1 Score, AUC-ROC
- ğŸ¯ **Regularization**: Dropout, L1/L2, Batch Normalization, EarlyStopping
- ğŸ“¦ **Embedding Layers**: How word embeddings work + practical examples
- ğŸ§ª **Model Training Tips**: Overfitting vs underfitting, callbacks, checkpoints

### ğŸ”¸ Frameworks & Tools

- âœ… **Keras/TensorFlow**: Model building, compiling, training, evaluation
- ğŸ§ª **Streamlit**: Interactive visual demos of concepts
- ğŸ“ **Jupyter Notebooks**: Interactive explanations and experiments
