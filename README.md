# 🧠 Deep Learning Concepts

This repository contains well-organized, annotated code and notes covering the foundational to advanced concepts in **Deep Learning**. It’s aimed at learners, practitioners, and anyone preparing for interviews or projects in the deep learning space.

---

## 📚 Contents

### 🔸 Core Topics

- 🔢 **Neural Networks**: Perceptron, Multi-layer Perceptron, Backpropagation
- 📐 **Activation Functions**: Sigmoid, Tanh, ReLU, LeakyReLU, Softmax
- 🧱 **Optimization**: Gradient Descent, Adam, RMSProp, Learning Rate Schedules
- 🧠 **CNNs**: Convolutional Layers, Pooling, Filters, CNN Architectures
- 🔁 **RNNs & LSTMs**: Sequence models, Vanishing gradients, LSTM/GRU internals
- 📉 **Loss Functions**: MSE, Cross-Entropy, Custom Losses
- 📊 **Evaluation Metrics**: Accuracy, Precision, Recall, F1 Score, AUC-ROC
- 🎯 **Regularization**: Dropout, L1/L2, Batch Normalization, EarlyStopping
- 📦 **Embedding Layers**: How word embeddings work + practical examples
- 🧪 **Model Training Tips**: Overfitting vs underfitting, callbacks, checkpoints

### 🔸 Frameworks & Tools

- ✅ **Keras/TensorFlow**: Model building, compiling, training, evaluation
- 🧪 **Streamlit**: Interactive visual demos of concepts
- 📁 **Jupyter Notebooks**: Interactive explanations and experiments
